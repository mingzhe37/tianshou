{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate expert dataset from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tianshou.data import ReplayBuffer, Batch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('pendulum_data_new.csv')\n",
    "\n",
    "# Initialize the Replay Buffer\n",
    "buffer_size = len(df)\n",
    "expert_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "# Populate the Replay Buffer\n",
    "for i in range(buffer_size):\n",
    "    obs = np.array(df.loc[i, 'observations'][1:-1].split(', '), dtype=float)\n",
    "    act = np.array([df.loc[i, 'actions']], dtype=float)\n",
    "    rew = df.loc[i, 'rewards']\n",
    "    done = df.loc[i, 'terminals']\n",
    "    next_obs = np.array(df.loc[i, 'next_observations'][1:-1].split(', '), dtype=float)\n",
    "    \n",
    "    # Assuming no truncation information is available, use the done flag for terminated\n",
    "    # and False for truncated (change this based on your specific case)\n",
    "    terminated = done\n",
    "    truncated = False\n",
    "\n",
    "    expert_buffer.add(\n",
    "        Batch(\n",
    "            obs=obs,\n",
    "            act=act,\n",
    "            rew=rew,\n",
    "            done=done,\n",
    "            obs_next=next_obs,\n",
    "            terminated=terminated,\n",
    "            truncated=truncated\n",
    "        )\n",
    "    )\n",
    "print(\"Data loaded into Tianshou Replay Buffer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data format in csv file (obsolete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and save the data\n",
    "transition_data = []\n",
    "for i in range(len(expert_buffer)):\n",
    "    transition = expert_buffer[i]\n",
    "    print(\"--- Transition {} ---\".format(i))\n",
    "    print(transition.obs)\n",
    "    print(transition.act)\n",
    "    print(transition.rew)\n",
    "    print(transition.done)\n",
    "    print(transition.obs_next)\n",
    "    print(\"--- End of Transition {} ---\".format(i))\n",
    "    transition_data.append(\n",
    "        Batch(\n",
    "            obs=transition.obs,\n",
    "            act=transition.act,\n",
    "            rew=transition.rew,\n",
    "            done=transition.done,\n",
    "            obs_next=transition.obs_next\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataset to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test saving the data to a pickle file\n",
    "with open(\"test_expert_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(expert_buffer, f)\n",
    "    print(\"Data saved to test_expert_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert_buffer._meta\n",
    "# print(expert_buffer._meta)\n",
    "print(expert_buffer._meta.__dict__.keys())\n",
    "print(expert_buffer._reserved_keys)\n",
    "# expert_buffer_act = expert_buffer._meta.__dict__['act'].astype(np.float32)\n",
    "# print(expert_buffer_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Check data format in read from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tianshou.data import ReplayBuffer, Batch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_buffer(file_name):\n",
    "    try:\n",
    "        with open(file_name, \"rb\") as f:\n",
    "            buffer = pickle.load(f)\n",
    "        print(\"Loaded expert buffer from {}\".format(file_name))\n",
    "        return buffer\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found: {}\".format(file_name))\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while loading the buffer: {}\".format(e))\n",
    "        return None\n",
    "\n",
    "# Specify the file name directly\n",
    "# file_name = \"expert_SAC_Pendulum-v1.pkl\"\n",
    "file_name = \"expert_MPC_JModelicaCSSingleZoneEnv-action-v2.pkl\"\n",
    "\n",
    "# Load the buffer\n",
    "buffer = load_buffer(file_name)\n",
    "\n",
    "# Print the attributes of the buffer\n",
    "print(buffer.maxsize)\n",
    "print(buffer._meta.__dict__.keys())\n",
    "print(buffer._meta.__dict__['obs'].astype(np.float32))\n",
    "\n",
    "# Chech the range of observations, maximum and minimum values in obeervations\n",
    "print(np.max(buffer._meta.__dict__['obs'].astype(np.float32)))\n",
    "print(np.min(buffer._meta.__dict__['obs'].astype(np.float32)))\n",
    "\n",
    "# Plot the first element of the observations\n",
    "# plt.plot(buffer._meta.__dict__['obs'].astype(np.float32)[:,0])\n",
    "# plt.show()\n",
    "\n",
    "# Plot the rewards\n",
    "plt.plot(buffer._meta.__dict__['rew'].astype(np.float32))\n",
    "plt.show()\n",
    "\n",
    "# Sum up the rewards\n",
    "print(np.sum(buffer._meta.__dict__['rew'].astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tianshou.data import ReplayBuffer\n",
    "\n",
    "# Load the pickle file\n",
    "# file_name = \"expert_SAC_Pendulum-v1.pkl\"  # Replace with your pickle file name\n",
    "# file_name = \"expert_MPC_JModelicaCSSingleZoneEnv-action-v2.pkl\"\n",
    "file_name = \"expert_buffer_MPC_JModelicaCSSingleZoneEnv-action-v2.pkl\"\n",
    "\n",
    "with open(file_name, \"rb\") as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "# Initialize the Replay Buffer\n",
    "buffer_size = len(loaded_data)\n",
    "expert_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "# Populate the Replay Buffer\n",
    "for data in loaded_data:\n",
    "    # If data is already a Batch object or similar\n",
    "    expert_buffer.add(data)\n",
    "    # If data is not a Batch object, you need to convert it. Example:\n",
    "    # expert_buffer.add(\n",
    "    #     Batch(\n",
    "    #         obs=data['obs'],\n",
    "    #         act=data['act'],\n",
    "    #         rew=data['rew'],\n",
    "    #         done=data['done'],\n",
    "    #         obs_next=data['obs_next'],\n",
    "    #         terminated=data.get('terminated', data['done']),  # Use 'done' if 'terminated' is not available\n",
    "    #         truncated=data.get('truncated', False)  # Default to False if not available\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "print(\"Data loaded into Tianshou Replay Buffer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.utils.statistics import RunningMeanStd\n",
    "rms = RunningMeanStd()\n",
    "\n",
    "# # Update running mean and standard deviation based on the observations\n",
    "# obs_array = expert_buffer._meta.__dict__['obs']  # Extract observations as ndarray\n",
    "# rms.update(obs_array)\n",
    "# # Normalize the observations\n",
    "# normalized_obs = rms.norm(obs_array)\n",
    "# # Replace the original observations with normalized observations\n",
    "# expert_buffer._meta.__dict__['obs'] = normalized_obs\n",
    "\n",
    "# # Update running mean and standard deviation based on the next observations\n",
    "# obs_next_array = expert_buffer._meta.__dict__['obs_next']  # Extract observations as ndarray\n",
    "# rms.update(obs_next_array)\n",
    "# # Normalize the observations\n",
    "# normalized_obs_next = rms.norm(obs_next_array)\n",
    "# # Replace the original observations with normalized observations\n",
    "# expert_buffer._meta.__dict__['obs_next'] = normalized_obs_next\n",
    "\n",
    "\n",
    "# Print the attributes of the buffer\n",
    "print(expert_buffer._meta.__dict__.keys())\n",
    "# expert_buffer._meta.__dict__['obs_next'][-1][0] = 86400.0\n",
    "# print(expert_buffer._meta.__dict__['obs'].astype(np.float32))\n",
    "print(expert_buffer._meta.__dict__['obs'].astype(np.float32))\n",
    "# print(len(expert_buffer._meta.__dict__['obs'].astype(np.float32)))\n",
    "# print(len(expert_buffer._meta.__dict__['obs_next'].astype(np.float32)))\n",
    "\n",
    "\n",
    "# Calculate the total reward\n",
    "print(np.sum(expert_buffer._meta.__dict__['rew'].astype(np.float32)))\n",
    "\n",
    "# Save the expert buffer to a pickle file with customised name\n",
    "# file_name = \"expert_buffer_MPC_JModelicaCSSingleZoneEnv-action-v2.pkl\"\n",
    "# with open(file_name, \"wb\") as f:\n",
    "#     pickle.dump(expert_buffer, f)\n",
    "#     print(\"Data saved to {}\".format(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test normalization of time index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = np.arange(900, 86400, 900)\n",
    "print(sequence)\n",
    "rms = RunningMeanStd()\n",
    "rms.update(sequence)\n",
    "normalized_sequence = rms.norm(sequence)\n",
    "print(normalized_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test saving the data to a pickle file\n",
    "with open(\"test_expert_dataset_Pendulum-v1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(expert_buffer, f)\n",
    "    print(\"Data saved to test_expert_dataset_Pendulum-v1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate vectortized reply buffer from pickle file (Need fix, since saved pickle file can not be used in gail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.data import VectorReplayBuffer, ReplayBuffer\n",
    "import pickle\n",
    "\n",
    "# Load the pickle file\n",
    "file_name = \"expert_SAC_Pendulum-v1.pkl\"  # Replace with your pickle file name\n",
    "with open(file_name, \"rb\") as f:\n",
    "    original_vector_buffer = pickle.load(f)\n",
    "\n",
    "# Initialize a new VectorReplayBuffer with the same number of buffers and buffer size\n",
    "num_buffers = len(original_vector_buffer.buffers)\n",
    "print(\"Number of buffers: {}\".format(num_buffers))\n",
    "buffer_size = len(original_vector_buffer)\n",
    "print(\"Buffer size: {}\".format(buffer_size))\n",
    "new_vector_buffer = VectorReplayBuffer(buffer_size, num_buffers)\n",
    "\n",
    "# Populate the new VectorReplayBuffer\n",
    "for buf_index, buf in enumerate(original_vector_buffer.buffers):\n",
    "    for transition in buf:\n",
    "        # Add directly to the corresponding buffer\n",
    "        new_vector_buffer.buffers[buf_index].add(transition)\n",
    "\n",
    "print(\"Data loaded into new VectorReplayBuffer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of buffers\n",
    "# num_buffers = 16\n",
    "\n",
    "# Initialize individual replay buffers\n",
    "# Assuming all buffers have the same size, e.g., 1000\n",
    "# buffers = [original_vector_buffer for _ in range(num_buffers)]\n",
    "\n",
    "# Create a VectorReplayBuffer instance with the list of buffers\n",
    "# vector_buffer = VectorReplayBuffer(buffers, buffer_num=num_buffers)\n",
    "\n",
    "# Now the vector_buffer is ready to use\n",
    "# print(vector_buffer)\n",
    "\n",
    "new_vector_buffer\n",
    "# original_vector_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vector_buffer\n",
    "\n",
    "# Print the attributes of the buffer\n",
    "print(new_vector_buffer.buffers[15]._meta.__dict__.keys())\n",
    "# print(new_vector_buffer._meta.__dict__['obs'].astype(np.float32))\n",
    "\n",
    "# print('====Fowlloing is for checking====')\n",
    "# # Example: Inspecting the first few transitions in each sub-buffer\n",
    "# for buf_index, buf in enumerate(new_vector_buffer.buffers):\n",
    "#     print(f\"Buffer {buf_index}:\")\n",
    "#     for i in range(min(len(buf), 5)):  # Print first 5 transitions of each buffer\n",
    "#         transition = buf[i]\n",
    "#         print(f\"  Transition {i}: {transition}\")\n",
    "#     print()  # Blank line for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test saving the data to a pickle file\n",
    "with open(\"test_vector_expert_dataset_Pendulum-v1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(new_vector_buffer, f)\n",
    "    print(\"Data saved to test_vector_expert_dataset_Pendulum-v1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read expert dataset from saved pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# from tianshou.data import VectorReplayBuffer  # Uncomment if needed\n",
    "\n",
    "# Path to your pickle file containing the VectorReplayBuffer\n",
    "pickle_file_path = 'expert_SAC_Pendulum-v1.pkl'\n",
    "\n",
    "# Load the buffer from the pickle file\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    buffer = pickle.load(file) # vectorized replay buffer, a VectorReplayBuffer object (used for the next two cells)\n",
    "    buffer = buffer.buffers[0] # One of the replay buffers in the vectorized replay buffer (used for this cell only)\n",
    "\n",
    "# Print the buffer metadata\n",
    "print(dir(buffer)) \n",
    "\n",
    "# Print the keys of the metadata\n",
    "print(buffer._meta.__dict__.keys())\n",
    "\n",
    "# Check and print if done and truncated and terminated are all False, print the index that is True\n",
    "# for i in range(len(buffer)):\n",
    "#     if buffer._meta.__dict__['done'][i] or buffer._meta.__dict__['truncated'][i] or buffer._meta.__dict__['terminated'][i]:\n",
    "#         print(i)\n",
    "#         print(buffer._meta.__dict__['done'][i])\n",
    "#         print(buffer._meta.__dict__['truncated'][i])\n",
    "#         print(buffer._meta.__dict__['terminated'][i])\n",
    "#         break\n",
    "\n",
    "# Print the index of true in the done, truncated and terminated\n",
    "print(np.where(buffer._meta.__dict__['done']))\n",
    "print(np.where(buffer._meta.__dict__['truncated']))\n",
    "print(np.where(buffer._meta.__dict__['terminated']))\n",
    "\n",
    "# Print all transitions when done is True\n",
    "for i in range(len(buffer)):\n",
    "    if buffer._meta.__dict__['done'][i]:\n",
    "        print(\"--- Transition {} ---\".format(i))\n",
    "        print(buffer._meta.__dict__['obs'][i])\n",
    "        print(buffer._meta.__dict__['act'][i])\n",
    "        print(buffer._meta.__dict__['rew'][i])\n",
    "        print(buffer._meta.__dict__['done'][i])\n",
    "        print(buffer._meta.__dict__['obs_next'][i])\n",
    "        print(\"--- End of Transition {} ---\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is based on vectorized replay buffer objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using __len__ to get the current size of the buffer\n",
    "buffer_size = len(buffer)\n",
    "print(f\"Current Size of the Buffer: {buffer_size}\")\n",
    "\n",
    "# Using maxsize to get the maximum capacity of the buffer\n",
    "buffer_capacity = buffer.maxsize\n",
    "print(f\"Maximum Capacity of the Buffer: {buffer_capacity}\")\n",
    "\n",
    "# Additional metadata\n",
    "buffer_num = buffer.buffer_num\n",
    "print(f\"Number of Buffers: {buffer_num}\")\n",
    "\n",
    "stack_num = buffer.stack_num\n",
    "print(f\"Number of Stacked Frames: {stack_num}\")\n",
    "\n",
    "# Access the first buffer (if it's a VectorReplayBuffer)\n",
    "single_buffer = buffer.buffers[0]\n",
    "\n",
    "# Check if the buffer is not empty\n",
    "if len(single_buffer) > 0:\n",
    "    # Accessing the first transition\n",
    "    # This depends on how the data is structured in your buffer\n",
    "    # For example, if transitions are stored in a list-like structure\n",
    "    first_transition = single_buffer[0]\n",
    "\n",
    "    # Now, print the contents of the first transition\n",
    "    print(\"First Transition Contents:\")\n",
    "    for key, value in first_transition.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"The buffer is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the expert dataset in a single buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a specific buffer from the VectorReplayBuffer\n",
    "specific_buffer = buffer.buffers[0]\n",
    "\n",
    "# Now you can print this buffer or inspect its contents\n",
    "print(\"Specific Buffer:\", specific_buffer)\n",
    "\n",
    "# To inspect details of this buffer, you can use dir() or access its attributes\n",
    "# For example, printing the size of this specific buffer\n",
    "print(\"Size of the Specific Buffer:\", len(specific_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rein",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
